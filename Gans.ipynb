{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('fashion-mnist_train.csv')\n",
    "train = train.drop(['label'],axis=1).to_numpy().reshape(-1,28,28,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices({'image':train})\n",
    "\n",
    "def preprocess(records):\n",
    "    images =  records['image']\n",
    "    images = tf.cast(images, tf.float32)/255.0\n",
    "    return images\n",
    "\n",
    "dataset = dataset.map(preprocess)\n",
    "dataset = dataset.repeat().shuffle(60000).batch(100).prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (28,28,1)\n",
    "final_encoder_dim = 2\n",
    "depth = 5\n",
    "kernel_size = 3\n",
    "activation = 'tanh'\n",
    "dropout = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import *\n",
    "from tensorflow.keras.layers import *\n",
    "\n",
    "def discriminator(input_shape, dim, depth, kernel, dropout,activation):\n",
    "    layers = []\n",
    "    layers.append(InputLayer(input_shape=input_shape))\n",
    "    for i in range(1,depth):\n",
    "        layers.append(Conv2D(16*i,kernel_size=kernel_size))\n",
    "        layers.append(BatchNormalization())\n",
    "        layers.append(Activation('relu'))\n",
    "        layers.append(Dropout(dropout))\n",
    "    layers.append(Flatten())\n",
    "    layers.append(Dense(128,activation='relu'))\n",
    "    layers.append(Dense(dim))\n",
    "    return Sequential(layers)\n",
    "\n",
    "encoder = discriminator(input_shape, final_encoder_dim, depth, kernel_size, dropout,activation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 26, 26, 16)        160       \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 26, 26, 16)        64        \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 26, 26, 16)        0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 26, 26, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 24, 24, 32)        4640      \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 24, 24, 32)        128       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 24, 24, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 24, 24, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 22, 22, 48)        13872     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 22, 22, 48)        192       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 22, 22, 48)        0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 22, 22, 48)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 20, 20, 64)        27712     \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 20, 20, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 20, 20, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 20, 20, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 25600)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               3276928   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 258       \n",
      "=================================================================\n",
      "Total params: 3,324,210\n",
      "Trainable params: 3,323,890\n",
      "Non-trainable params: 320\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(input_shape, depth, output_shape,kernel,dropout):\n",
    "    layers = []\n",
    "    layers.append(InputLayer(input_shape=(input_shape,)))\n",
    "    layers.append(Dense(784,activation='relu'))\n",
    "    layers.append(Reshape(target_shape=output_shape))\n",
    "    for i in range(1,depth):\n",
    "        layers.append(Conv2DTranspose(16*i,kernel_size=kernel))\n",
    "        layers.append(BatchNormalization())\n",
    "        layers.append(Activation('relu'))\n",
    "        layers.append(Dropout(dropout))\n",
    "    \n",
    "    resizer =  lambda name: Lambda(lambda images: tf.image.resize(images, [28,28]), name=name)\n",
    "    layers.append(resizer('Reshape'))\n",
    "    layers.append(Conv2DTranspose(1,kernel_size=1,activation=None))\n",
    "    return Sequential(layers)\n",
    "decoder = generator(final_encoder_dim, depth, input_shape,kernel_size,dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_2 (Dense)              (None, 784)               2352      \n",
      "_________________________________________________________________\n",
      "reshape (Reshape)            (None, 28, 28, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose (Conv2DTran (None, 30, 30, 16)        160       \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 30, 30, 16)        64        \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 30, 30, 16)        0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 30, 30, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_1 (Conv2DTr (None, 32, 32, 32)        4640      \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 32, 32, 32)        128       \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_2 (Conv2DTr (None, 34, 34, 48)        13872     \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 34, 34, 48)        192       \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 34, 34, 48)        0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 34, 34, 48)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_3 (Conv2DTr (None, 36, 36, 64)        27712     \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 36, 36, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 36, 36, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 36, 36, 64)        0         \n",
      "_________________________________________________________________\n",
      "Reshape (Lambda)             (None, 28, 28, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_4 (Conv2DTr (None, 28, 28, 1)         65        \n",
      "=================================================================\n",
      "Total params: 49,441\n",
      "Trainable params: 49,121\n",
      "Non-trainable params: 320\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "decoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "encoder_opt = tf.keras.optimizers.Adam()\n",
    "decoder_opt = tf.keras.optimizers.Adam()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def training(training_data):\n",
    "    batch_size = tf.shape(training_data)[0]\n",
    "    real_data = training_data\n",
    "    real_labels = tf.ones((batch_size,))\n",
    "    fake_labels = tf.zeros((batch_size,))\n",
    "    labels = tf.concat((real_labels,fake_labels),axis=0)\n",
    "    noise = tf.random.normal((batch_size,final_encoder_dim))\n",
    "    \n",
    "    with tf.GradientTape() as decoder_gt, tf.GradientTape() as encoder_gt:\n",
    "        \n",
    "        fake_images = decoder(noise,training=True)\n",
    "        fake_labels_2 = encoder(fake_images,training=True)\n",
    "        real_labels_2 = encoder(training_data,training=True)\n",
    "        predicted_labels = tf.concat((real_labels_2,fake_labels_2),axis=0)\n",
    "        \n",
    "\n",
    "        discrim_loss = loss(labels,predicted_labels)\n",
    "        #print(discrim_loss)\n",
    "        gen_loss = loss(real_labels,fake_labels_2)\n",
    "        \n",
    "    dec_grad = decoder_gt.gradient(gen_loss,decoder.trainable_variables)\n",
    "    enc_grad = encoder_gt.gradient(discrim_loss,encoder.trainable_variables)\n",
    "    \n",
    "    decoder_opt.apply_gradients(zip(dec_grad, decoder.trainable_variables))\n",
    "    encoder_opt.apply_gradients(zip(enc_grad, encoder.trainable_variables))\n",
    "    \n",
    "    return discrim_loss, gen_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "def epoch_training(data_iterator, steps_per_epoch, avg_gen_loss, avg_dis_loss):\n",
    "    for x in range(steps_per_epoch):\n",
    "        d_loss, g_loss = training(next(data_iterator))\n",
    "        avg_gen_loss.update_state(g_loss)\n",
    "        avg_dis_loss.update_state(d_loss)\n",
    "        if x%20==0:\n",
    "            print('{} steps done'.format(x))\n",
    "            \n",
    "    gen_loss = avg_gen_loss.result()\n",
    "    dis_loss = avg_dis_loss.result()\n",
    "    \n",
    "    \n",
    "    tf.summary.scalar('gen_loss',gen_loss,step=encoder_opt.iterations)\n",
    "    tf.summary.flush()\n",
    "    tf.summary.scalar('dis_loss',dis_loss,step=decoder_opt.iterations)\n",
    "    tf.summary.flush()\n",
    "    \n",
    "    avg_gen_loss.reset_state()\n",
    "    avg_dis_loss.reset_state()\n",
    "    return gen_loss.numpy(), dis_loss.numpy()\n",
    "    \n",
    "def train(training_data, epochs):\n",
    "    \n",
    "\n",
    "    \n",
    "    checkpoint_dir = './training_checkpoints'\n",
    "    checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "    checkpoint = tf.train.Checkpoint(generator_optimizer=decoder_opt,discriminator_optimizer=encoder_opt,generator=decoder,discriminator=encoder)\n",
    "    manager = tf.train.CheckpointManager(checkpoint, 'training_checkpoints', max_to_keep=5)\n",
    "    checkpoint.restore(manager.latest_checkpoint)\n",
    "    \n",
    "    summary = tf.summary.create_file_writer('metrics/train')\n",
    "    \n",
    "    avg_generator_loss = tf.keras.metrics.Mean()\n",
    "    avg_discriminator_loss = tf.keras.metrics.Mean()\n",
    "    \n",
    "    data_iterator = training_data.__iter__()\n",
    "    for i in range(epochs):\n",
    "        with summary.as_default():\n",
    "            gen_loss, dis_loss = epoch_training(data_iterator, 600, avg_generator_loss, avg_discriminator_loss)\n",
    "            print({'gen_loss':gen_loss,'dis_loss':dis_loss})\n",
    "            manager.save()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 steps done\n",
      "20 steps done\n",
      "40 steps done\n",
      "60 steps done\n",
      "80 steps done\n",
      "100 steps done\n",
      "120 steps done\n",
      "140 steps done\n",
      "160 steps done\n",
      "180 steps done\n",
      "200 steps done\n",
      "220 steps done\n",
      "240 steps done\n",
      "260 steps done\n",
      "280 steps done\n",
      "300 steps done\n",
      "320 steps done\n",
      "340 steps done\n",
      "360 steps done\n",
      "380 steps done\n",
      "400 steps done\n",
      "420 steps done\n",
      "440 steps done\n",
      "460 steps done\n",
      "480 steps done\n",
      "500 steps done\n",
      "520 steps done\n",
      "540 steps done\n",
      "560 steps done\n",
      "580 steps done\n",
      "{'gen_loss': 5.1994023, 'dis_loss': 0.14459148}\n",
      "0 steps done\n",
      "20 steps done\n",
      "40 steps done\n",
      "60 steps done\n",
      "80 steps done\n",
      "100 steps done\n",
      "120 steps done\n",
      "140 steps done\n",
      "160 steps done\n",
      "180 steps done\n",
      "200 steps done\n",
      "220 steps done\n",
      "240 steps done\n",
      "260 steps done\n",
      "280 steps done\n",
      "300 steps done\n",
      "320 steps done\n",
      "340 steps done\n",
      "360 steps done\n",
      "380 steps done\n",
      "400 steps done\n",
      "420 steps done\n",
      "440 steps done\n",
      "460 steps done\n",
      "480 steps done\n",
      "500 steps done\n",
      "520 steps done\n",
      "540 steps done\n",
      "560 steps done\n",
      "580 steps done\n",
      "{'gen_loss': 7.0065413, 'dis_loss': 0.05855007}\n",
      "0 steps done\n",
      "20 steps done\n",
      "40 steps done\n",
      "60 steps done\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-c15e4435f818>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-11-3bcc669bd715>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(training_data, epochs)\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0msummary\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m             \u001b[0mgen_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdis_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mepoch_training\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_iterator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m600\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mavg_generator_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mavg_discriminator_loss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     42\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'gen_loss'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mgen_loss\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'dis_loss'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mdis_loss\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m             \u001b[0mmanager\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-11-3bcc669bd715>\u001b[0m in \u001b[0;36mepoch_training\u001b[1;34m(data_iterator, steps_per_epoch, avg_gen_loss, avg_dis_loss)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mepoch_training\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_iterator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mavg_gen_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mavg_dis_loss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m         \u001b[0md_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mg_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtraining\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_iterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m         \u001b[0mavg_gen_loss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate_state\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mg_loss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m         \u001b[0mavg_dis_loss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate_state\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md_loss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-10-acb5cea6e4f5>\u001b[0m in \u001b[0;36mtraining\u001b[1;34m(training_data)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[0mdec_grad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdecoder_gt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgen_loss\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m     \u001b[0menc_grad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mencoder_gt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdiscrim_loss\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mencoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[0mdecoder_opt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdec_grad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\backprop.py\u001b[0m in \u001b[0;36mgradient\u001b[1;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[0;32m   1082\u001b[0m                           for x in nest.flatten(output_gradients)]\n\u001b[0;32m   1083\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1084\u001b[1;33m     flat_grad = imperative_grad.imperative_grad(\n\u001b[0m\u001b[0;32m   1085\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tape\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1086\u001b[0m         \u001b[0mflat_targets\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\imperative_grad.py\u001b[0m in \u001b[0;36mimperative_grad\u001b[1;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[0m\n\u001b[0;32m     69\u001b[0m         \"Unknown value for unconnected_gradients: %r\" % unconnected_gradients)\n\u001b[0;32m     70\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 71\u001b[1;33m   return pywrap_tfe.TFE_Py_TapeGradient(\n\u001b[0m\u001b[0;32m     72\u001b[0m       \u001b[0mtape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tape\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m       \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\backprop.py\u001b[0m in \u001b[0;36m_gradient_function\u001b[1;34m(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads, skip_input_indices, forward_pass_name_scope)\u001b[0m\n\u001b[0;32m    157\u001b[0m       \u001b[0mgradient_name_scope\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mforward_pass_name_scope\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"/\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    158\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgradient_name_scope\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 159\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    160\u001b[0m   \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    161\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\nn_grad.py\u001b[0m in \u001b[0;36m_Conv2DBackpropInputGrad\u001b[1;34m(op, grad)\u001b[0m\n\u001b[0;32m     53\u001b[0m           \u001b[0muse_cudnn_on_gpu\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_attr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"use_cudnn_on_gpu\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m           data_format=op.get_attr(\"data_format\").decode()),\n\u001b[1;32m---> 55\u001b[1;33m       gen_nn_ops.conv2d(\n\u001b[0m\u001b[0;32m     56\u001b[0m           \u001b[0mgrad\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m           \u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_nn_ops.py\u001b[0m in \u001b[0;36mconv2d\u001b[1;34m(input, filter, strides, padding, use_cudnn_on_gpu, explicit_paddings, data_format, dilations, name)\u001b[0m\n\u001b[0;32m    922\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mtld\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_eager\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    923\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 924\u001b[1;33m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[0m\u001b[0;32m    925\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Conv2D\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"strides\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstrides\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    926\u001b[0m         \u001b[1;34m\"use_cudnn_on_gpu\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muse_cudnn_on_gpu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"padding\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(dataset,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise = tf.random.normal([1,2])\n",
    "images = decoder(noise,training=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "for x in images:\n",
    "    plt.imshow(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "count =0\n",
    "for x in dataset:\n",
    "    \n",
    "    plt.imshow(x[1])\n",
    "    if count==5:\n",
    "        break\n",
    "    count+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
